{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<H1><center>Neural Architecture Search using MCGRAN</center></H1>"
      ],
      "metadata": {
        "id": "9JrMWRR58WDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-conditional GRAN"
      ],
      "metadata": {
        "id": "R8Brb9S19Nfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We model the conditional graph generation as an affine transformation based on given contraints.\n",
        "We use $ \\textrm{MLP}_{scale} $ and $ \\textrm{MLP}_{shift} $ to geometrically transform the feature space of bernoulli mixture components.\n",
        "Our intention with this transformation is to separate the various constraints(classes or categories) of graphs into distinct real space.\n",
        "\n",
        "\\begin{align}\n",
        "\t\\alpha_{1},..., \\alpha_{K} = \\textrm{Softmax}(\\sum_{i \\in \\boldsymbol{b}_{t},1 \\leq j \\leq i} \\textrm{MLP}_{\\alpha} (h_{i}^{R} - h_{j}^{R}) \\otimes \\textrm{MLP}_{scale}(c_{i}) + \\textrm{MLP}_{shift}(c_{i}))\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\t\\theta_{1,i,j},..., \\theta_{K,i,j} = \\textrm{Sigmoid}(\\textrm{MLP}_{\\theta} (h_{i}^{R} - h_{j}^{R})  \\otimes \\textrm{MLP}_{scale}(c_{i}) + \\textrm{MLP}_{shift}(c_{i}))\n",
        "\\end{align}\n",
        "\n",
        "In equations, $ M $ denote the number of constraints enforced on each node $ i $, $ c_{i} \\in \\mathbb{R}^{M} $ represents the constraints vector associated with each node $ i $, $ \\textrm{MLP}_{scale} \\in  \\mathbb{R}^{K \\times H}$ is a RELU-based hidden layer capturing features for scaling factor, and $ \\textrm{MLP}_{shift} \\in  \\mathbb{R}^{K \\times H} $ is a RELU-based hidden layer capturing features for shift factor. $ K $ denote the number of mixture components and $ H $ denote the hidden dimension size."
      ],
      "metadata": {
        "id": "UtAUMszI-KMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph-based Auto-regressive Affine Transformations"
      ],
      "metadata": {
        "id": "AlTfISfM_yn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the structure of the graph, that is, the adjacency matrix $ A $ for each graph $G = (V,E)$. We create node level features using 1D convolutions. We use a block of three 1D convolutional hidden layers stacked together to capture features for each vertices $ v $, where $ \\textrm{CNN}(A): \\mathbb{R}^{|V| \\times |V|} \\rightarrow \\mathbb{R}^{|V| \\times H}$, where adjacency matrix denoted as $ A \\in \\mathbb{R}^{|V| \\times |V|} $. Each channel of convolution captures the information specific to a node.\n",
        "\n",
        "Then, we apply auto-regressive affine transformation to create node labels.  In graph-based auto-regressive affine transformations, for each node label prediction, we use three information.  First information comes from the node features which we captured using the CNN. Let us denote node features for a node $v\\in V$ as $q_v\\in\\mathbb{R}^{H}$ with $H$ being the feature dimension.\n",
        "\n",
        "Second information comes from the nodes features of the other connected nodes. This acts as neighbor constraints to predict the current node label. For a node $v\\in V$ of a graph $G = (V,E)$ its set of neighboring features is given as\n",
        "\\[\n",
        "Neigh(v) = \\{ q_s ~|~ (s,v) \\in E \\}\n",
        "\\]\n",
        "We could also define $NAgg(v): \\mathbb{R}^{|Neigh(v)| \\times H} \\rightarrow \\mathbb{R}^{H}$\n",
        "\n",
        "\\begin{align}\n",
        "\tNAgg(v) = \\sum\\limits_{q\\in\\{ q_s ~|~ (s,v) \\in E \\}}\n",
        "\\end{align}\n",
        "\n",
        "For each node $ v $, we apply affine transformation based on node neighbors with one scaling factor $ \\textrm{MLP}_{scale}(NAgg(v)) $ and one shift factor $ \\textrm{MLP}_{shift}(NAgg(v)) $. \n",
        "\n",
        "Third information comes from the graph level constraint. We apply geometric transformation based on graph constraints with one scaling factor  $ \\textrm{MLP}_{scale}(c_{v}) $ and one shift factor $ \\textrm{MLP}_{shift}(c_{v}) $.\n",
        "\n",
        "The $ c_{v} \\in \\mathbb{R}^{M} $ represents the constraints vector associated with each node $ v $, the $ M $ denote the number of constraints enforced on each node $ v $, the $ \\textrm{MLP}_{scale} \\in  \\mathbb{R}^{H}$ is a RELU-based hidden layer capturing features for scaling factor, and the $ \\textrm{MLP}_{shift} \\in  \\mathbb{R}^{ H} $ is a RELU-based hidden layer capturing features for shift factor. The $ H $ denote the hidden dimension size.\n",
        "\n",
        "%From a social network perspective, each person (node) uses the information he or she knows, then collects information from his or her friends, and finally collects information other sources (books or internet) to make a decision.\n",
        "%The same process is applied here.\n",
        "\n",
        "We can combine all three information to predict for each node $ v $ the label as shown in equation~\\ref{eq:mcgran_node_label_equation}.\n",
        "\n",
        "\\begin{align}\n",
        "\tv = \\textrm{Softmax}(\\textrm{CNN}(v) \\otimes \\textrm{MLP}_{scale}(NAgg(v)) \\otimes \\textrm{MLP}_{scale}(c_{v})   + \\textrm{MLP}_{shift}(NAgg(v)) + \\textrm{MLP}_{shift}(c_{v}))\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "fh8FJykCAA1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhRxOd1qivxo",
        "outputId": "0efe8722-76a3-4101-e727-ceebb081109d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113"
      ],
      "metadata": {
        "id": "HPIwYuQBBGf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20a3c30-fa92-49de-f811-9be8616beb64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[K     |████████████████▎               | 834.1 MB 1.2 MB/s eta 0:11:18tcmalloc: large alloc 1147494400 bytes == 0x39b32000 @  0x7fb54d1fe615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x4fdff5 0x49caa1 0x4fdff5 0x49ced5 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x5d7c18 0x5d9412 0x586636 0x5d813c 0x55f3fd 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x5d7c18 0x49ca7c 0x4fdff5 0x49ced5\n",
            "\u001b[K     |████████████████████▋           | 1055.7 MB 1.2 MB/s eta 0:07:57tcmalloc: large alloc 1434370048 bytes == 0x7e188000 @  0x7fb54d1fe615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x4fdff5 0x49caa1 0x4fdff5 0x49ced5 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x5d7c18 0x5d9412 0x586636 0x5d813c 0x55f3fd 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x5d7c18 0x49ca7c 0x4fdff5 0x49ced5\n",
            "\u001b[K     |██████████████████████████▏     | 1336.2 MB 1.2 MB/s eta 0:04:15tcmalloc: large alloc 1792966656 bytes == 0x2fba000 @  0x7fb54d1fe615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x4fdff5 0x49caa1 0x4fdff5 0x49ced5 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x5d7c18 0x5d9412 0x586636 0x5d813c 0x55f3fd 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x5d7c18 0x49ca7c 0x4fdff5 0x49ced5\n",
            "\u001b[K     |████████████████████████████████| 1637.0 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1636999168 bytes == 0x6dda2000 @  0x7fb54d1fd1e7 0x4d3280 0x4d330c 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ced5 0x4fdff5 0x49ced5 0x55e571\n",
            "tcmalloc: large alloc 2046255104 bytes == 0xcf6cc000 @  0x7fb54d1fe615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ced5 0x4fdff5 0x49ced5 0x55e571 0x5d7cf1 0x4fea58\n",
            "\u001b[K     |████████████████████████████████| 1637.0 MB 10 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp38-cp38-linux_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0+cu113) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.12.0+cu113) (2022.9.24)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.0+cu116\n",
            "    Uninstalling torchaudio-0.13.0+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchvision-0.12.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dn1buYZHWGw",
        "outputId": "cd8e1a36-67f9-4d21-e396-a7d30a114232"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w9MZa_zMF4o3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0802b31-1479-4dca-f1f7-4b2962729816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  498M  100  498M    0     0   152M      0  0:00:03  0:00:03 --:--:--  152M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://storage.googleapis.com/nasbench/nasbench_only108.tfrecord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6b80AUMxHaLc"
      },
      "outputs": [],
      "source": [
        "#!g++ -O2 -std=c++11 -o drive/MyDrive/Research-NAS/MCGRAN/utils/orca/orca.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "80BIYLdU27qm"
      },
      "outputs": [],
      "source": [
        "#!chmod +x drive/MyDrive/Research-NAS/MCGRAN/utils/orca/orca"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4RW8Zz8COrZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration Settings"
      ],
      "metadata": {
        "id": "_TZqIwxeCR0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the configurable settings for working the GRAN model are specified in the yaml files located in the config folder.  There are five sections of parameters in the configuration files:\n",
        "\n",
        "1.   General experimental parameters\n",
        "2.   Dataset parameters\n",
        "3.   Model parameters\n",
        "4.   Training parameters\n",
        "5.   Test parameters"
      ],
      "metadata": {
        "id": "njK-vrjtCbbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General experimental parameters"
      ],
      "metadata": {
        "id": "iNQDSbHvEOVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name of the experiment\n",
        "> **exp_name**: *MCGRAN*\n",
        "\n",
        "The experiment directory folder name if not already present.  This folder will contain the training and evaluation metrics.\n",
        "\n",
        "> **exp_dir**: *exp/MCGRAN/* \n",
        "\n",
        "Name of the runner class name from gran_runner_*.py file\n",
        "> **runner**: *GranRunner_Evaluation*\n",
        "\n",
        "Distributed training of the model in multiple machines. Always set to false.  We did not test with true.\n",
        "> **use_horovod**: *false*\n",
        "\n",
        "GRU related settings. Always set to true. we did not test with false.\n",
        "> **use_gpu**: *true*      \n",
        "\n",
        "Cuda device id\n",
        "> **device**: *cuda:0*  \n",
        "\n",
        "Number of GPUs\n",
        "> **gpus**: [0]\n",
        "\n",
        "Random seed for reproducing the experiments\n",
        "> seed: 78123456"
      ],
      "metadata": {
        "id": "ygg4c_c3ESS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python drive/MyDrive/Research-NAS/MCGRAN/run_exp.py -c drive/MyDrive/Research-NAS/MCGRAN/config/mcgran.yaml -t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slxomCDMHHYw",
        "outputId": "062da337-6a95-4e38-ac7f-c30c92cd2b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from file... This may take a few minutes...\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/Research-NAS/MCGRAN/nasbench/api.py:146: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/Research-NAS/MCGRAN/nasbench/api.py:146: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "Loaded dataset in 44 seconds\n",
            "INFO  | 2022-12-08 12:21:14,435 | run_exp.py                | line 31   : Writing log file to /content/drive/MyDrive/Research-NAS/MCGRAN/exp/MCGRAN/503/log_exp_503.txt\n",
            "INFO  | 2022-12-08 12:21:14,437 | run_exp.py                | line 32   : Exp instance id = 503\n",
            "INFO  | 2022-12-08 12:21:14,438 | run_exp.py                | line 33   : Exp comment = None\n",
            "INFO  | 2022-12-08 12:21:14,438 | run_exp.py                | line 34   : Config =\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "{'dataset': {'data_path': './',\n",
            "             'dev_ratio': 0.2,\n",
            "             'has_node_feat': False,\n",
            "             'is_overwrite_precompute': False,\n",
            "             'is_sample_subgraph': True,\n",
            "             'is_save_split': True,\n",
            "             'loader_name': 'GRANData_Targeted_Search',\n",
            "             'max_num_samples': 1000,\n",
            "             'max_test_accuracy': 100,\n",
            "             'min_test_accuracy': 80,\n",
            "             'name': 'nas',\n",
            "             'node_order': 'none',\n",
            "             'num_fwd_pass': 2,\n",
            "             'num_subgraph_batch': 2,\n",
            "             'train_ratio': 0.8},\n",
            " 'device': 'cuda:0',\n",
            " 'exp_dir': '/content/drive/MyDrive/Research-NAS/MCGRAN/exp/MCGRAN/',\n",
            " 'exp_name': '503',\n",
            " 'free_cache': False,\n",
            " 'gpus': [0],\n",
            " 'model': {'block_size': 1,\n",
            "           'dimension_reduce': True,\n",
            "           'display_detailed_model': False,\n",
            "           'edge_weight': 1.0,\n",
            "           'embedding_dim': 256,\n",
            "           'has_attention': True,\n",
            "           'hidden_dim': 256,\n",
            "           'is_sym': False,\n",
            "           'max_num_nodes': 7,\n",
            "           'name': 'MCGRAN',\n",
            "           'node_categories': 3,\n",
            "           'num_GNN_layers': 7,\n",
            "           'num_GNN_prop': 1,\n",
            "           'num_canonical_order': 1,\n",
            "           'num_mix_component': 64,\n",
            "           'sample_stride': 1},\n",
            " 'run_id': '503',\n",
            " 'runner': 'GranRunner_Evaluation',\n",
            " 'save_dir': '/content/drive/MyDrive/Research-NAS/MCGRAN/exp/MCGRAN/503',\n",
            " 'seed': 78123456,\n",
            " 'test': {'batch_size': 100,\n",
            "          'better_vis': True,\n",
            "          'generated_graph_data_dir': 'generated_data',\n",
            "          'is_single_plot': False,\n",
            "          'is_test_ER': False,\n",
            "          'is_vis': True,\n",
            "          'num_test_gen': 100,\n",
            "          'num_vis': 10,\n",
            "          'num_workers': 0,\n",
            "          'test_model_dir': '/content/drive/MyDrive/ICLR2023/MCGRAN/exp/MCGRAN/58724',\n",
            "          'test_model_name': 'model_snapshot_0000281.pth',\n",
            "          'vis_num_row': 1},\n",
            " 'train': {'batch_size': 20,\n",
            "           'display_iter': 10,\n",
            "           'is_resume': False,\n",
            "           'lr': 0.0001,\n",
            "           'lr_decay': 0.1,\n",
            "           'lr_decay_epoch': [100000000],\n",
            "           'max_epoch': 38,\n",
            "           'momentum': 0.9,\n",
            "           'num_workers': 0,\n",
            "           'optimizer': 'Adam',\n",
            "           'resume_dir': None,\n",
            "           'resume_epoch': 100,\n",
            "           'resume_model': None,\n",
            "           'shuffle': True,\n",
            "           'snapshot_epoch': 38,\n",
            "           'valid_epoch': 50,\n",
            "           'wd': 0.0},\n",
            " 'use_gpu': True,\n",
            " 'use_horovod': False}\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "2022-12-08 12:21:22.404835: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO  | 2022-12-08 12:21:23,277 | gran_runner_evaluation.py | line 113  : Train/val/test = 800/200/200\n",
            "800\n",
            "INFO  | 2022-12-08 12:21:23,283 | gran_runner_evaluation.py | line 154  : No Edges vs. Edges in training set = 4.5721393034825875\n",
            "Training Loss @ epoch 0001; Loss: 1.5752; Accuracy: 0.5111; Precision: 0.5840; Recall: 0.5811; F1 Score: 0.5789\n",
            "Training Loss @ epoch 0002; Loss: 1.3312; Accuracy: 0.5416; Precision: 0.6107; Recall: 0.6098; F1 Score: 0.5889\n",
            "Training Loss @ epoch 0003; Loss: 1.1537; Accuracy: 0.5548; Precision: 0.6275; Recall: 0.6217; F1 Score: 0.6087\n",
            "Training Loss @ epoch 0004; Loss: 1.0039; Accuracy: 0.5498; Precision: 0.6203; Recall: 0.6188; F1 Score: 0.6124\n",
            "Training Loss @ epoch 0005; Loss: 1.0801; Accuracy: 0.5580; Precision: 0.6313; Recall: 0.6247; F1 Score: 0.6125\n",
            "Training Loss @ epoch 0006; Loss: 1.0540; Accuracy: 0.5654; Precision: 0.6346; Recall: 0.6326; F1 Score: 0.6294\n",
            "Training Loss @ epoch 0007; Loss: 1.0657; Accuracy: 0.5732; Precision: 0.6428; Recall: 0.6385; F1 Score: 0.6332\n",
            "Training Loss @ epoch 0008; Loss: 1.0512; Accuracy: 0.5807; Precision: 0.6480; Recall: 0.6457; F1 Score: 0.6427\n",
            "Training Loss @ epoch 0009; Loss: 1.0274; Accuracy: 0.5918; Precision: 0.6574; Recall: 0.6557; F1 Score: 0.6549\n",
            "Training Loss @ epoch 0010; Loss: 1.0404; Accuracy: 0.5950; Precision: 0.6605; Recall: 0.6576; F1 Score: 0.6554\n",
            "Training Loss @ epoch 0011; Loss: 0.9994; Accuracy: 0.6005; Precision: 0.6643; Recall: 0.6631; F1 Score: 0.6619\n",
            "Training Loss @ epoch 0012; Loss: 1.0123; Accuracy: 0.6045; Precision: 0.6669; Recall: 0.6661; F1 Score: 0.6640\n",
            "Training Loss @ epoch 0013; Loss: 1.0718; Accuracy: 0.6118; Precision: 0.6735; Recall: 0.6731; F1 Score: 0.6726\n",
            "Training Loss @ epoch 0014; Loss: 1.0001; Accuracy: 0.6139; Precision: 0.6754; Recall: 0.6748; F1 Score: 0.6747\n",
            "Training Loss @ epoch 0015; Loss: 0.9978; Accuracy: 0.6223; Precision: 0.6820; Recall: 0.6818; F1 Score: 0.6813\n",
            "Training Loss @ epoch 0016; Loss: 0.9689; Accuracy: 0.6298; Precision: 0.6892; Recall: 0.6882; F1 Score: 0.6882\n",
            "Training Loss @ epoch 0017; Loss: 1.0477; Accuracy: 0.6325; Precision: 0.6907; Recall: 0.6907; F1 Score: 0.6903\n",
            "Training Loss @ epoch 0018; Loss: 1.0407; Accuracy: 0.6380; Precision: 0.6956; Recall: 0.6952; F1 Score: 0.6948\n",
            "Training Loss @ epoch 0019; Loss: 0.9556; Accuracy: 0.6512; Precision: 0.7066; Recall: 0.7065; F1 Score: 0.7065\n",
            "Training Loss @ epoch 0020; Loss: 0.9879; Accuracy: 0.6561; Precision: 0.7105; Recall: 0.7105; F1 Score: 0.7102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "358pOC156qnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir 'drive/MyDrive/GRAN/exp/GRAN_EVALUATION'"
      ],
      "metadata": {
        "id": "yzgRql3X6rTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "sPnRNrYOCG6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "base_path = \"drive/MyDrive/GRAN/exp/GRAN_EVALUATION\"\n",
        "experiment_folder_list = os.listdir(base_path)"
      ],
      "metadata": {
        "id": "sjH0z6ScwxKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created 8 model with different random seeds and collected the data for the below evaluation metrics.\n",
        "\n",
        "*   Self-loops\n",
        "*   Isolated Nodes\n",
        "*   Validity of generated graphs that satisfy constraints of Neural Network\n",
        "*   Maximum Mean Discrepancy(MMD) of number of nodes, node degree, node clustering, and graph spectrum\n",
        "*   Test Accuracy of generated neural networks\n",
        "*   Uniqueness of generated neural networks\n",
        "*   Novelty of generated neural networks\n",
        "*   Visual analysis of general neural network"
      ],
      "metadata": {
        "id": "IC6io0__vgaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-loops\n",
        "\n",
        "A self-loop check determines if the node connects to itself. For neural architecture, we don't expect the layers to connect to themselves. We will count the graphs which have self-loops."
      ],
      "metadata": {
        "id": "ei6_IlqewZ2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "structure_evaluation_metrics_list = list()\n",
        "for folder_name in experiment_folder_list:\n",
        "    folder_name = os.path.join(base_path, folder_name)\n",
        "    file_name = os.path.join(folder_name, 'structure_evaluation_metrics.p')\n",
        "    if os.path.exists(file_name):\n",
        "        structure_evaluation_metrics = pickle.load(open(file_name, 'rb'))\n",
        "        structure_evaluation_metrics_list.append(structure_evaluation_metrics)"
      ],
      "metadata": {
        "id": "3qWwcZIXw0_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_loops_list = list()\n",
        "for metrics in structure_evaluation_metrics_list:\n",
        "    self_loops_list.append(metrics['self_loops'])\n",
        "self_loops_list = np.array(self_loops_list)\n",
        "print(\"Maximum number of graphs with self-loops \", np.max(self_loops_list))\n",
        "print(\"Minimum number of graphs with self-loops \", np.min(self_loops_list))\n",
        "print(\"Average number of graphs with self-loops \", np.mean(self_loops_list))\n",
        "print(\"Standard deviation of graphs with self-loops \", np.std(self_loops_list))"
      ],
      "metadata": {
        "id": "R3Zfv_ycw9X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Isolated Nodes\n",
        "\n",
        "Isolated nodes check will identify nodes not connected to any other in the graph. In a neural network, we expect the layers to form connections with another layer. We will count the graphs with isolated nodes."
      ],
      "metadata": {
        "id": "FdQmGz5cwiIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "isolated_nodes_list = list()\n",
        "for metrics in structure_evaluation_metrics_list:\n",
        "    isolated_nodes_list.append(metrics['isolated_nodes'])\n",
        "isolated_nodes_list = np.array(isolated_nodes_list)\n",
        "print(\"Maximum number of graphs with isolated nodes \", np.max(isolated_nodes_list))\n",
        "print(\"Minimum number of graphs with isolated nodes \", np.min(isolated_nodes_list))\n",
        "print(\"Average number of graphs with isolated nodes \", np.mean(isolated_nodes_list))\n",
        "print(\"Standard deviation of graphs with isolated nodes \", np.std(isolated_nodes_list))"
      ],
      "metadata": {
        "id": "WVpdi14sxYJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validity\n",
        "\n",
        "Validity measures the number of validly generated graphs. Specifically, it has to satisfy the constraints of Directed Acyclic Graph and Neural Networks.\n",
        "\n",
        "*   Only the input node is at the start.\n",
        "*   Only the output node is at the end.\n",
        "*   Except for the output node, there should not be any node without a successor.\n",
        "*   Except for the input node, there should not be any node without a predecessor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rSyTE9oVxgHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_nn_list = list()\n",
        "for metrics in structure_evaluation_metrics_list:\n",
        "    invalid_nn_list.append(metrics['invalid_nn'])\n",
        "invalid_nn_list = np.array(invalid_nn_list)\n",
        "print(\"Maximum number of invalid graphs generated \", np.max(invalid_nn_list))\n",
        "print(\"Minimum number of invalid graphs generated \", np.min(invalid_nn_list))\n",
        "print(\"Average number of invalid graphs generated \", np.mean(invalid_nn_list))\n",
        "print(\"Standard deviation of invalid graphs generated \", np.std(invalid_nn_list))"
      ],
      "metadata": {
        "id": "ybZ39INoyPVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.boxplot([invalid_nn_list],\n",
        "           labels=['Invalid graphs'])"
      ],
      "metadata": {
        "id": "j-MHba9JGTrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Maximum Mean Discrepancy\n",
        "\n",
        "Maximum Mean Discrepancy(MMD) compares two probability distributions and reports how far they are apart. We evaluate the following metrics only on valid graphs in order to eliminate the discrepancy caused by invalid graphs.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\operatorname{MMD}^{2}(p \\| q) =\\mathbb{E}_{x, y \\sim p}[k(x, y)] + \\mathbb{E}_{x, y \\sim q}[k(x, y)] - 2 \\mathbb{E}_{x \\sim p, y \\sim q}[k(x, y)]\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "lDgp8ufeyUdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Validation_MMD_Scores_Validation_vs_Generated_Graphs_list = list()\n",
        "for folder_name in experiment_folder_list:\n",
        "    folder_name = os.path.join(base_path, folder_name)\n",
        "    file_name = os.path.join(folder_name, 'Validation_MMD_Scores_Validation_vs_Generated_Graphs.p')\n",
        "    if os.path.exists(file_name):\n",
        "        Validation_MMD_Scores_Validation_vs_Generated_Graphs = pickle.load(open(file_name, 'rb'))\n",
        "        Validation_MMD_Scores_Validation_vs_Generated_Graphs_list.append(Validation_MMD_Scores_Validation_vs_Generated_Graphs)"
      ],
      "metadata": {
        "id": "8TrfGiBny5tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MMD_num_of_nodes_list = list()\n",
        "for metrics in Validation_MMD_Scores_Validation_vs_Generated_Graphs_list:\n",
        "    MMD_num_of_nodes_list.append(metrics[0])\n",
        "MMD_num_of_nodes_list = np.array(MMD_num_of_nodes_list)\n",
        "print(\"Maximum MMD distance between number of nodes in the validation and generated set \", np.max(MMD_num_of_nodes_list))\n",
        "print(\"Minimum MMD distance between number of nodes in the validation and generated set \", np.min(MMD_num_of_nodes_list))\n",
        "print(\"Average MMD distance between number of nodes in the validation and generated set \", np.mean(MMD_num_of_nodes_list))\n",
        "print(\"Standard deviation of MMD distance between number of nodes in the validation and generated set \", np.std(MMD_num_of_nodes_list))"
      ],
      "metadata": {
        "id": "_Mhk_OoEzC7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MMD_node_degree_list = list()\n",
        "for metrics in Validation_MMD_Scores_Validation_vs_Generated_Graphs_list:\n",
        "    MMD_node_degree_list.append(metrics[1])\n",
        "MMD_node_degree_list = np.array(MMD_node_degree_list)\n",
        "print(\"Maximum MMD distance between node degree in the validation and generated set \", np.max(MMD_node_degree_list))\n",
        "print(\"Minimum MMD distance between node degree in the validation and generated set \", np.min(MMD_node_degree_list))\n",
        "print(\"Average MMD distance between node degree in the validation and generated set \", np.mean(MMD_node_degree_list))\n",
        "print(\"Standard deviation of MMD distance between node degree in the validation and generated set \", np.std(MMD_node_degree_list))"
      ],
      "metadata": {
        "id": "6EDxMO3Zz4J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MMD_node_clustering_list = list()\n",
        "for metrics in Validation_MMD_Scores_Validation_vs_Generated_Graphs_list:\n",
        "    MMD_node_clustering_list.append(metrics[2])\n",
        "MMD_node_clustering_list = np.array(MMD_node_clustering_list)\n",
        "print(\"Maximum MMD distance between node clustering in the validation and generated set \", np.max(MMD_node_clustering_list))\n",
        "print(\"Minimum MMD distance between node clustering in the validation and generated set \", np.min(MMD_node_clustering_list))\n",
        "print(\"Average MMD distance between node clustering in the validation and generated set \", np.mean(MMD_node_clustering_list))\n",
        "print(\"Standard deviation of MMD distance between node clustering in the validation and generated set \", np.std(MMD_node_clustering_list))"
      ],
      "metadata": {
        "id": "g0RiLncZz-vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MMD_node_orbits_list = list()\n",
        "for metrics in Validation_MMD_Scores_Validation_vs_Generated_Graphs_list:\n",
        "    MMD_node_orbits_list.append(metrics[3])\n",
        "MMD_node_orbits_list = np.array(MMD_node_orbits_list)\n",
        "print(\"Maximum MMD distance between node orbit count in the validation and generated set \", np.max(MMD_node_orbits_list))\n",
        "print(\"Minimum MMD distance between node orbit count in the validation and generated set \", np.min(MMD_node_orbits_list))\n",
        "print(\"Average MMD distance between node orbit count in the validation and generated set \", np.mean(MMD_node_orbits_list))\n",
        "print(\"Standard deviation of MMD distance between node orbit count in the validation and generated set \", np.std(MMD_node_orbits_list))"
      ],
      "metadata": {
        "id": "e3Tg2yTH0DIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MMD_graph_spectrum_list = list()\n",
        "for metrics in Validation_MMD_Scores_Validation_vs_Generated_Graphs_list:\n",
        "    MMD_graph_spectrum_list.append(metrics[4])\n",
        "MMD_graph_spectrum_list = np.array(MMD_graph_spectrum_list)\n",
        "print(\"Maximum MMD distance between graph laplacian in the validation and generated set \", np.max(MMD_graph_spectrum_list))\n",
        "print(\"Minimum MMD distance between graph laplacian in the validation and generated set \", np.min(MMD_graph_spectrum_list))\n",
        "print(\"Average MMD distance between graph laplacian in the validation and generated set \", np.mean(MMD_graph_spectrum_list))\n",
        "print(\"Standard deviation of MMD distance between graph laplacian in the validation and generated set \", np.std(MMD_graph_spectrum_list))"
      ],
      "metadata": {
        "id": "0xN8lXJK0IZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Accuracy\n",
        "\n",
        "Let us review the the test accuracy of the valid graphs. In the following list, each value is the mean of the test accuracy of the generated graphs for the GRAN model generated with a random seed."
      ],
      "metadata": {
        "id": "hN9tI9oQ0N7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_list = list()\n",
        "for folder_name in experiment_folder_list:\n",
        "    folder_name = os.path.join(base_path, folder_name)\n",
        "    file_name = os.path.join(folder_name, 'generated_graph_test_accuracy_list.p')\n",
        "    if os.path.exists(file_name):\n",
        "        generated_graph_test_accuracy_list = pickle.load(open(file_name, 'rb'))\n",
        "        test_accuracy_list.append(sum(generated_graph_test_accuracy_list)/len(generated_graph_test_accuracy_list))\n",
        "test_accuracy_list = np.array(test_accuracy_list)\n",
        "print(\"Maximum test accuracy of valid graphs generated \", np.max(test_accuracy_list))\n",
        "print(\"Minimum test accuracy of valid graphs generated \", np.min(test_accuracy_list))\n",
        "print(\"Average test accuracy of valid graphs generated \", np.mean(test_accuracy_list))\n",
        "print(\"Standard deviation of the test accuracy of valid graphs generated \", np.std(test_accuracy_list))"
      ],
      "metadata": {
        "id": "MM3cz-9Q0UlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Uniqueness\n",
        "\n",
        "Uniqueness measures the unique share of the generated graphs. Simply put, we calculate the graph similarity. If the generated graphs are diverse among the generated graphs, then the generative model produces unique graphs.\n",
        "\n",
        "Graph edit distance measure tells us the minimum of edits in nodes and edges required in a graph to produce a corresponding isomorphic graph."
      ],
      "metadata": {
        "id": "4Tzqjb8P0dcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueness_list = list()\n",
        "for metrics in structure_evaluation_metrics_list:\n",
        "    if 'uniqueness' in metrics.keys():\n",
        "        uniqueness_list.append(metrics['uniqueness'])\n",
        "\n",
        "graph_edit_distance_distribution = dict()\n",
        "for metric in uniqueness_list:\n",
        "    for key in metric.keys():\n",
        "        if key in graph_edit_distance_distribution.keys():\n",
        "            graph_edit_distance_distribution[key] += metric[key]\n",
        "        else:\n",
        "            graph_edit_distance_distribution[key] = metric[key]\n",
        "\n",
        "for key in graph_edit_distance_distribution.keys():\n",
        "    graph_edit_distance_distribution[key] = graph_edit_distance_distribution[key]/8 # 8 represent the number of evaluations\n",
        "\n",
        "for key in sorted(graph_edit_distance_distribution):\n",
        "    print(f\"{key}: {graph_edit_distance_distribution[key]}\")"
      ],
      "metadata": {
        "id": "TheqUuTP0ohG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Novelty\n",
        "\n",
        "We measure novelty as the valid number of generated graphs not available in the training dataset.\n",
        "\n",
        "As defined earlier, graph edit distance measure the number of changes required to make one graph isomorphic to another graph. The graph edit distance metric will measure the novelty as the number of changes in the generated graphs to create the same graph in the training set. Therefore, the more the graph edit distance, the more the novelty of the graph generation model."
      ],
      "metadata": {
        "id": "kr4V9LWJ030C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novelty_list = list()\n",
        "for metrics in structure_evaluation_metrics_list:\n",
        "    if 'novelty' in metrics.keys():\n",
        "        novelty_list.append(metrics['novelty'])\n",
        "graph_edit_distance_distribution = dict()\n",
        "for metric in novelty_list:\n",
        "    for key in metric.keys():\n",
        "        if key in graph_edit_distance_distribution.keys():\n",
        "            graph_edit_distance_distribution[key] += metric[key]\n",
        "        else:\n",
        "            graph_edit_distance_distribution[key] = metric[key]\n",
        "for key in graph_edit_distance_distribution.keys():\n",
        "    graph_edit_distance_distribution[key] = graph_edit_distance_distribution[key]/8 # 8 represent the number of evaluations\n",
        "for key in sorted(graph_edit_distance_distribution):\n",
        "    print(f\"{key}: {graph_edit_distance_distribution[key]}\")"
      ],
      "metadata": {
        "id": "aGwpMexz0co5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visual Analysis using PyDot\n",
        "\n",
        "As the visual analysis of the all the graphs in this notebook is not possible. We provide a simple interface to visualize the generated graphs."
      ],
      "metadata": {
        "id": "m4M87gui5lsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import SVG\n",
        "selected_experiment_folder_name = 'GRANMixtureBernoulli_Evaluation_nas_2022-Jan-03-11-41-31_30198'\n",
        "selected_experiment_folder_name = os.path.join(base_path, selected_experiment_folder_name)"
      ],
      "metadata": {
        "id": "-Y1SNDv-51Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_graph_name = \"architecture_3.svg\" # graphs are numbered from 0 to 99\n",
        "file_name = os.path.join(selected_experiment_folder_name, selected_graph_name)\n",
        "SVG(file_name)"
      ],
      "metadata": {
        "id": "6vfhxJ0D54qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_graph_name = \"architecture_4.svg\" # graphs are numbered from 0 to 99\n",
        "file_name = os.path.join(selected_experiment_folder_name, selected_graph_name)\n",
        "SVG(file_name)"
      ],
      "metadata": {
        "id": "HXSxXZz86Axj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}